{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4887d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 30 records from page 1\n",
      "Scraped 60 records from page 2\n",
      "Scraped 90 records from page 3\n",
      "Scraped 120 records from page 4\n",
      "Scraped 150 records from page 5\n",
      "Scraped 180 records from page 6\n",
      "Scraped 210 records from page 7\n",
      "Scraped 240 records from page 8\n",
      "Scraped 270 records from page 9\n",
      "Scraped 300 records from page 10\n",
      "Scraped 330 records from page 11\n",
      "Scraped 360 records from page 12\n",
      "Scraped 390 records from page 13\n",
      "Scraped 420 records from page 14\n",
      "Scraped 449 records from page 15\n",
      "Scraped 479 records from page 16\n",
      "Scraped 508 records from page 17\n",
      "Scraped 538 records from page 18\n",
      "Scraped 568 records from page 19\n",
      "Scraped 598 records from page 20\n",
      "Scraped 628 records from page 21\n",
      "Scraped 658 records from page 22\n",
      "Scraped 687 records from page 23\n",
      "Scraped 717 records from page 24\n",
      "Scraped 747 records from page 25\n",
      "Scraped 777 records from page 26\n",
      "Scraped 807 records from page 27\n",
      "Scraped 837 records from page 28\n",
      "Scraped 867 records from page 29\n",
      "Scraped 897 records from page 30\n",
      "Scraped 927 records from page 31\n",
      "Scraped 957 records from page 32\n",
      "Scraped 987 records from page 33\n",
      "Scraped 1017 records from page 34\n",
      "Scraped 1047 records from page 35\n",
      "Scraped 1077 records from page 36\n",
      "Scraped 1107 records from page 37\n",
      "Scraped 1137 records from page 38\n",
      "Scraped 1167 records from page 39\n",
      "Scraped 1196 records from page 40\n",
      "Scraped 1226 records from page 41\n",
      "Scraped 1256 records from page 42\n",
      "Scraped 1286 records from page 43\n",
      "Scraped 1316 records from page 44\n",
      "Scraped 1346 records from page 45\n",
      "Scraped 1375 records from page 46\n",
      "Scraped 1405 records from page 47\n",
      "Scraped 1435 records from page 48\n",
      "Scraped 1465 records from page 49\n",
      "Scraped 1495 records from page 50\n",
      "Scraped 1523 records from page 51\n",
      "Scraped 1553 records from page 52\n",
      "Scraped 1582 records from page 53\n",
      "Scraped 1611 records from page 54\n",
      "Scraped 1641 records from page 55\n",
      "Scraped 1671 records from page 56\n",
      "Scraped 1701 records from page 57\n",
      "Scraped 1731 records from page 58\n",
      "Scraped 1761 records from page 59\n",
      "Scraped 1791 records from page 60\n",
      "Scraped 1819 records from page 61\n",
      "Scraped 1848 records from page 62\n",
      "Scraped 1877 records from page 63\n",
      "Scraped 1907 records from page 64\n",
      "Scraped 1937 records from page 65\n",
      "Scraped 1967 records from page 66\n",
      "Scraped 1997 records from page 67\n",
      "Scraped 2027 records from page 68\n",
      "Scraped 2057 records from page 69\n",
      "Scraped 2087 records from page 70\n",
      "Scraped 2117 records from page 71\n",
      "Scraped 2147 records from page 72\n",
      "Scraped 2177 records from page 73\n",
      "Scraped 2206 records from page 74\n",
      "Scraped 2236 records from page 75\n",
      "Scraped 2266 records from page 76\n",
      "Scraped 2296 records from page 77\n",
      "Scraped 2326 records from page 78\n",
      "Scraped 2356 records from page 79\n",
      "Scraped 2386 records from page 80\n",
      "Scraped 2416 records from page 81\n",
      "Scraped 2446 records from page 82\n",
      "Scraped 2476 records from page 83\n",
      "Scraped 2506 records from page 84\n",
      "Scraped 2536 records from page 85\n",
      "Scraped 2566 records from page 86\n",
      "Scraped 2596 records from page 87\n",
      "Scraped 2626 records from page 88\n",
      "Scraped 2656 records from page 89\n",
      "Scraped 2684 records from page 90\n",
      "Scraped 2712 records from page 91\n",
      "Scraped 2741 records from page 92\n",
      "Scraped 2770 records from page 93\n",
      "Scraped 2799 records from page 94\n",
      "Scraped 2828 records from page 95\n",
      "Scraped 2857 records from page 96\n",
      "Scraped 2887 records from page 97\n",
      "Scraped 2916 records from page 98\n",
      "Scraped 2945 records from page 99\n",
      "Scraped 2975 records from page 100\n",
      "Scraped 3005 records from page 101\n",
      "Scraped 3035 records from page 102\n",
      "Scraped 3065 records from page 103\n",
      "Scraped 3095 records from page 104\n",
      "Scraped 3125 records from page 105\n",
      "Scraped 3155 records from page 106\n",
      "Scraped 3185 records from page 107\n",
      "Scraped 3215 records from page 108\n",
      "Scraped 3245 records from page 109\n",
      "Scraped 3275 records from page 110\n",
      "Scraped 3305 records from page 111\n",
      "Scraped 3335 records from page 112\n",
      "Scraped 3365 records from page 113\n",
      "Scraped 3395 records from page 114\n",
      "Scraped 3425 records from page 115\n",
      "Scraped 3455 records from page 116\n",
      "Scraped 3485 records from page 117\n",
      "Scraped 3515 records from page 118\n",
      "                              location bhk total_floors carpet_area  \\\n",
      "0                        Kesnand, Pune   1         None    375 sqft   \n",
      "1                        Bavdhan, Pune   2         None        None   \n",
      "2                 Shukrawar Peth, Pune   2         None    938 sqft   \n",
      "3                          Wakad, Pune   2         None    816 sqft   \n",
      "4     Punawale, Pimpri Chinchwad, Pune   4         None   1777 sqft   \n",
      "...                                ...  ..          ...         ...   \n",
      "3510                      Pimpri, Pune   2         None    706 sqft   \n",
      "3511                     Wanwadi, Pune   2         None    673 sqft   \n",
      "3512                   Hinjawadi, Pune   2         None    783 sqft   \n",
      "3513                    Balewadi, Pune   3         None   1223 sqft   \n",
      "3514           Kiwale, Dehu Road, Pune   3         None   1009 sqft   \n",
      "\n",
      "     super_area     price            rate  \\\n",
      "0          None    34 Lac   7816 per sqft   \n",
      "1      828 sqft   1.09 Cr  13164 per sqft   \n",
      "2          None   1.75 Cr  13823 per sqft   \n",
      "3          None  90.9 Lac   7796 per sqft   \n",
      "4          None   2.22 Cr  10039 per sqft   \n",
      "...         ...       ...             ...   \n",
      "3510       None    83 Lac            None   \n",
      "3511       None    84 Lac            None   \n",
      "3512       None    89 Lac            None   \n",
      "3513       None   1.76 Cr  10669 per sqft   \n",
      "3514       None  86.8 Lac            None   \n",
      "\n",
      "                            developer_name  \n",
      "0                              Gagan Aviva  \n",
      "1                 Puravankara Purva Aspire  \n",
      "2                             Chandrakamal  \n",
      "3                                 MJ Opera  \n",
      "4                            ANP Autograph  \n",
      "...                                    ...  \n",
      "3510                    Kohinoor Shangrila  \n",
      "3511  Paradise Cooperative Housing Society  \n",
      "3512                             Sensorium  \n",
      "3513                        Dream Elegance  \n",
      "3514                               Aryaban  \n",
      "\n",
      "[3515 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \n",
    "    driver_path = 'C:/Users/Lenovo/Downloads/geckodriver-v0.34.0-win32/geckodriver.exe'\n",
    "\n",
    "    \n",
    "    firefox_binary_path = 'C:/Program Files/Mozilla Firefox/firefox.exe'\n",
    "\n",
    "    service = FirefoxService(executable_path=driver_path)\n",
    "    options = FirefoxOptions()\n",
    "    options.headless = True  \n",
    "\n",
    "   \n",
    "    options.binary_location = firefox_binary_path\n",
    "\n",
    "    return webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "\n",
    "def extract_data(soup):\n",
    "    data = []\n",
    "\n",
    "    property_containers = soup.find_all('div', class_='mb-srp__list')\n",
    "\n",
    "    for container in property_containers:\n",
    "        try:\n",
    "            title_element = container.find('h2', class_='mb-srp__card--title')\n",
    "            if title_element:\n",
    "                project_info = title_element.text.strip()\n",
    "                match = re.match(r'\\d+\\sBHK Flat\\s+for\\s+Sale\\s+in\\s+(.+)', project_info)\n",
    "                if match:\n",
    "                    project = match.group(1).strip()\n",
    "\n",
    "                   \n",
    "                    location = ', '.join(project.split(', ')[1:])\n",
    "                    \n",
    "                    project = project.split(', ')[0]\n",
    "\n",
    "                   \n",
    "                    bhk = None\n",
    "                    bhk_match = re.search(r'(\\d+)\\sBHK', project_info)\n",
    "                    if bhk_match:\n",
    "                        bhk = bhk_match.group(1)\n",
    "\n",
    "                    total_floors_element = container.find('span', {'data-summary': 'floor'})\n",
    "                    total_floors = total_floors_element.find('div', class_='mb-srp__card__summary--value').text.strip() if total_floors_element else None\n",
    "\n",
    "                    carpet_area_element = container.find('div', {'data-summary': 'carpet-area'})\n",
    "                    carpet_area = carpet_area_element.find('div', class_='mb-srp__card__summary--value').text.strip() if carpet_area_element else None\n",
    "\n",
    "                    super_area_element = container.find('div', {'data-summary': 'super-area'})\n",
    "                    super_area = super_area_element.find('div', class_='mb-srp__card__summary--value').text.strip() if super_area_element else None\n",
    "\n",
    "                    price_element = container.find('div', class_='mb-srp__card__price--amount')\n",
    "                    price = price_element.text.replace('₹', '').strip() if price_element else None\n",
    "\n",
    "                    rate_element = container.find('div', class_='mb-srp__card__price--size')\n",
    "                    rate = rate_element.text.replace('₹', '').strip() if rate_element else None\n",
    "\n",
    "                    # Handling different HTML structures for developer_name\n",
    "                    developer_name_element = container.find('span', class_='mb-srp__card__developer--name--highlight')\n",
    "                    if developer_name_element:\n",
    "                        developer_name = developer_name_element.text.strip()\n",
    "                    else:\n",
    "                        society_name_element = container.find('a', class_='mb-srp__card__society--name')\n",
    "                        developer_name = society_name_element.text.strip() if society_name_element else \"Unknown\"\n",
    "\n",
    "                    \n",
    "                    data.append({\n",
    "                        'project': project,\n",
    "                        'location': location,\n",
    "                        'bhk': bhk,\n",
    "                        'total_floors': total_floors,\n",
    "                        'carpet_area': carpet_area,\n",
    "                        'super_area': super_area,\n",
    "                        'price': price,\n",
    "                        'rate': rate,\n",
    "                        'developer_name': developer_name,\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def scrape_and_save_data(driver, url, target_records, output_path):\n",
    "    data = []\n",
    "    current_records = 0\n",
    "    page_number = 1\n",
    "\n",
    "    while current_records < target_records:\n",
    "        try:\n",
    "            current_url = f\"{url}&page={page_number}\"\n",
    "            driver.get(current_url)\n",
    "\n",
    "            \n",
    "            wait = WebDriverWait(driver, 120)\n",
    "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'mb-srp__card__container')))\n",
    "\n",
    "           \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            extracted_data = extract_data(soup)\n",
    "\n",
    "            if not extracted_data:\n",
    "                break  # Break if no more records are found on the current page\n",
    "\n",
    "            \n",
    "            current_records += len(extracted_data)\n",
    "\n",
    "            \n",
    "            data.extend(extracted_data)\n",
    "\n",
    "            print(f\"Scraped {current_records} records from page {page_number}\")\n",
    "            page_number += 1\n",
    "\n",
    "            \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)  # Add a delay to let the page load\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Retrying page {page_number}...\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "            continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "   \n",
    "    df['location'] = df['project'] + ', ' + df['location']\n",
    "\n",
    "    \n",
    "    df.drop(columns=['project'], inplace=True)\n",
    "\n",
    "   \n",
    "    df = df[['location', 'bhk', 'total_floors', 'carpet_area', 'super_area', 'price', 'rate', 'developer_name']]\n",
    "\n",
    "    \n",
    "    df.to_excel(output_path, index=False)\n",
    "\n",
    "   \n",
    "    print(df)\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://www.magicbricks.com/flats-in-pune-for-sale-pppfs?mbtracker=google_paid_brand_sitelink_pune&cCode=sem_brand_sitelink&gclid=Cj0KCQiAtOmsBhCnARIsAGPa5yaBq3zdZaTU7DVD_bDEeLInDGq7qKAX9CgpiR-3ZKE8dijMEZV7VeIaAsChEALw_wcB\"\n",
    "target_records = 3500\n",
    "output_path = 'C:/Users/Lenovo/Downloads/MohitGanvir_99acres.xlsx'\n",
    "\n",
    "driver = setup_driver()\n",
    "scrape_and_save_data(driver, url, target_records, output_path)\n",
    "\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "data = pd.read_excel('C:/Users/Lenovo/Downloads/MohitGanvir_99acres.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "data['price'] = data['price'].apply(lambda x: float(x.split()[0]) * 10000000 if 'Cr' in x else float(x.split()[0]) * 100000)\n",
    "\n",
    "data['total_floors'] = data['total_floors'].fillna(data['total_floors'].mean())\n",
    "data['carpet_area'] = data['carpet_area'].str.extract('(\\d+)').astype(float)\n",
    "data['super_area'] = data['super_area'].str.extract('(\\d+)').astype(float)\n",
    "data['super_area'] = np.where(data['super_area'].isnull(), data['carpet_area'], data['super_area'])\n",
    "data['price'] = data['price'].fillna(data['price'].mean())\n",
    "\n",
    "data['rate'] = data['rate'].replace('[^\\d.]', '', regex=True).astype(float)\n",
    "\n",
    "data['bhk'] = data['bhk'].astype(float)\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['project_encoded'] = le.fit_transform(data['developer_name'])\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features_imputed = imputer.fit_transform(data[['total_floors', 'carpet_area', 'super_area', 'rate', 'bhk', 'project_encoded']])\n",
    "target = data['price']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_imputed, target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "gradient_boosting_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "models = [linear_reg_model, random_forest_model, gradient_boosting_model]\n",
    "\n",
    "predictions_columns = []\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    model.fit(features_imputed, target)\n",
    "    \n",
    "    \n",
    "    predictions = model.predict(features_imputed)\n",
    "    \n",
    "    \n",
    "    col_name = f'{model.__class__.__name__}_Prediction'\n",
    "    data[col_name] = predictions\n",
    "    predictions_columns.append(col_name)\n",
    "\n",
    "\n",
    "output_file_path = 'C:/Users/Lenovo/Downloads/MohitGanvir_99acres_with_predictions.xlsx'\n",
    "data.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Predictions attached to the input file. Saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c850f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
